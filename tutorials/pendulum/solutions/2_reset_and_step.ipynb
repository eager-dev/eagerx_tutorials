{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4c98be-8431-4ded-aec2-32aa5179725e",
   "metadata": {},
   "source": [
    "# Tutorial 2: Reset and Step\n",
    "\n",
    "In this tutorial, we will show how to create a gym environment using [EAGERx](https://eagerx.readthedocs.io/en/master/).\n",
    "Here we go a bit deeper into the [step()](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.BaseEnv.step) and [reset()](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.BaseEnv.reset) methods.\n",
    "\n",
    "The following will be covered:\n",
    "- Extracting observations in the [step()](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.BaseEnv.step) method.\n",
    "- Resetting states using the [reset()](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv.reset) method.\n",
    "- The `window` argument of the [connect method](https://eagerx.readthedocs.io/en/master/guide/api_reference/graph/graph.html?highlight=connect#eagerx.core.graph.Graph.connect)\n",
    "- Simulating delays using the `delay` argument of the [connect()](https://eagerx.readthedocs.io/en/master/guide/api_reference/graph/graph.html?highlight=connect#eagerx.core.graph.Graph.connect) method.\n",
    "\n",
    "In the remainder of this tutorial, we will go more into detail on these concepts.\n",
    "\n",
    "Furthermore, at the end of this notebook you will find exercises.\n",
    "For the exercises you will have to add/modify a couple of lines of code, which are marked by\n",
    "\n",
    "```python\n",
    "\n",
    "# START EXERCISE [BLOCK_NUMBER]\n",
    "\n",
    "# END EXERCISE [BLOCK_NUMBER]\n",
    "```\n",
    "\n",
    "## Pendulum Swing-up\n",
    "\n",
    "We will create an environment for solving the classic control problem of swinging up an underactuated pendulum, very similar to the [Pendulum-v1 environment](https://www.gymlibrary.ml/environments/classic_control/pendulum/).\n",
    "Our goal is to swing up this pendulum to the upright position and keep it there, while minimizing the velocity of the pendulum and the input voltage.\n",
    "\n",
    "Since the dynamics of a pendulum actuated by a DC motor are well known, we can simulate the pendulum by integrating the corresponding ordinary differential equations (ODEs):\n",
    "\n",
    "\n",
    "$\\mathbf{x} = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix} \\\\ \\dot{\\mathbf{x}} = \\begin{bmatrix} \\dot{\\theta} \\\\ \\frac{1}{J}(\\frac{K}{R}u - mgl \\sin{\\theta} - b \\dot{\\theta} - \\frac{K^2}{R}\\dot{\\theta})\\end{bmatrix}$\n",
    "\n",
    "with $\\theta$ the angle w.r.t. upright position, $\\dot{\\theta}$ the angular velocity, $u$ the input voltage, $J$ the inertia, $m$ the mass, $g$ the gravitational constant, $l$ the length of the pendulum, $b$ the motor viscous friction constant, $K$ the motor constant and $R$ the electric resistance.\n",
    "\n",
    "\n",
    "\n",
    "## Activate GPU (Colab only)\n",
    "\n",
    "When in Colab, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "## Notebook Setup\n",
    "\n",
    "In order to be able to run the code, we need to install the *eagerx_tutorials* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "985d462b-fdc5-4539-bd18-a58eafd22353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import eagerx_tutorials\n",
    "except ImportError:\n",
    "    !{\"echo 'Installing eagerx-tutorials with pip.' && pip install eagerx-tutorials >> /tmp/eagerx_install.txt 2>&1\"}\n",
    "\n",
    "# Setup interactive notebook\n",
    "# Required in interactive notebooks only.\n",
    "from eagerx_tutorials import helper\n",
    "helper.setup_notebook()\n",
    "\n",
    "# Import eagerx\n",
    "import eagerx\n",
    "eagerx.set_log_level(eagerx.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df883914-af6b-4d82-a1a7-6045b70347ff",
   "metadata": {},
   "source": [
    "## Let's get started\n",
    "\n",
    "Next, we make the specification for a *Pendulum* object and add it to an empty graph, just like we did in the [first tutorial](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/1_environment_creation.ipynb).\n",
    "\n",
    "We will again connect the *u* actuator of the *Pendulum* to an action that we will call *voltage* and connect the sensors *theta* and *dtheta* to observations, which we will call *angle* and *angular_velocity*.\n",
    "However, we will now go a bit more into detail on the [connect method](https://eagerx.readthedocs.io/en/master/guide/api_reference/graph/graph.html?highlight=connect#eagerx.core.graph.Graph.connect).\n",
    "When connecting outputs, sensors or actions, we can specify among other things the `window` of the connection.\n",
    "It specifies how to deal with messages that are sent between nodes in between calls to their callback.\n",
    "In some cases it makes sense to use the last one only; in others you would like to receive all messages between calls.\n",
    "This can be achieved by setting the `window` size:\n",
    "\n",
    "- `window` $= 1$: Only the last received input message are available to the receiver.\n",
    "- `window` $= x \\ge 1$: The trailing last $x$ received input messages are available to the receiver ($1 \\le$ received number of messages $\\le$ `window` ).\n",
    "- `window` $= 0$: All input messages received since the last call to the node's callback are available.\n",
    "\n",
    "This is in particular relevant when connecting to observations, since it has consequences for the size of the observation space.\n",
    "When connecting to an observation with `window` $= 0$, this observation will **not by default** be included in the observation space of the agent, because its dimensions might change every time step and are therefore unknown beforehand. You can, however, always decide to add the observation manually to the space in the Gym environment definiton.\n",
    "Also worth noting, is that for observations with `window` $= x > 1$, at time step $t < x$, the first message is repeated $x - t$ times to ensure that the dimensions of the observation space is consistent.\n",
    "\n",
    "Besides the `window` size, we can also specify a `delay` for each connection.\n",
    "In this way, we can easily simulate delays for inputs and sensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca2e099b-1723-45d4-9ade-44ec3475002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rate (Hz)\n",
    "rate = 30.0\n",
    "\n",
    "# Initialize empty graph\n",
    "graph = eagerx.Graph.create()\n",
    "\n",
    "# Make pendulum\n",
    "from eagerx_tutorials.pendulum.objects import Pendulum\n",
    "\n",
    "# START EXERCISE 1.1\n",
    "# Remove sensor dtheta from and add sensor u_applied to the list of sensors.  \n",
    "# sensors = [\"theta\", \"dtheta\", \"image\"]\n",
    "sensors = [\"theta\", \"image\", \"u_applied\"]  # Solution\n",
    "# END EXERCISE 1.1\n",
    "\n",
    "# START EXERCISE 2.1\n",
    "# Add the state model_parameters to the list of states of the pendulum. \n",
    "# states = [\"model_state\"]\n",
    "states = [\"model_state\", \"model_parameters\"]  # Solution\n",
    "# END EXERCISE 2.1\n",
    "\n",
    "pendulum = Pendulum.make(\"pendulum\", actuators=[\"u\"], sensors=sensors, states=states)\n",
    "\n",
    "# Add pendulum to the graph\n",
    "graph.add(pendulum)\n",
    "\n",
    "# Connect the pendulum to an action and observation\n",
    "# We will now explicitly set the window size\n",
    "graph.connect(action=\"voltage\", target=pendulum.actuators.u, window=1)\n",
    "\n",
    "# START EXERCISE 1.2 & 1.3\n",
    "# 1.2 Remove the connection from sensor dtheta. \n",
    "# 1.3 Also, connect sensor theta with window=3 to stack the last three observations of theta and set delay to 0.01.\n",
    "# graph.connect(source=pendulum.sensors.theta, observation=\"angle\", window=1)\n",
    "# graph.connect(source=pendulum.sensors.dtheta, observation=\"angular_velocity\", window=1)\n",
    "graph.connect(source=pendulum.sensors.theta, observation=\"angle\", window=3, delay=0.01)  # Solution\n",
    "# END EXERCISE 1.2\n",
    "\n",
    "# START EXERCISE 1.4\n",
    "# Connect u_applied to an observation called voltage_applied with window=1.\n",
    "# Do you know why u_applied should be an observation to the agent in order to restore the Markov property?\n",
    "graph.connect(source=pendulum.sensors.u_applied, observation=\"voltage_applied\", window=1)  # Solution\n",
    "# END EXERCISE 1.4\n",
    "\n",
    "# Render image\n",
    "graph.render(source=pendulum.sensors.image, rate=rate)\n",
    "\n",
    "# Make OdeEngine\n",
    "from eagerx_ode.engine import OdeEngine\n",
    "engine = OdeEngine.make(rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad124293-588e-46b4-8f39-99f8461a10a0",
   "metadata": {},
   "source": [
    "Using the [*eagerx_gui* package](https://github.com/eager-dev/eagerx_gui), we see that the graph looks as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "graph.gui()\n",
    "```\n",
    "\n",
    "<img src=\"./figures/tutorial_1_gui.svg\" width=720>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690463d-1998-4095-9f7e-8eced4a4196c",
   "metadata": {},
   "source": [
    "Next, we will create the [Environment](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html).\n",
    "We will now define the [step()](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.BaseEnv.step) method.\n",
    "Here we define the `reward` and fill the `info` dictionary at each time step.\n",
    "Since we want to stabilize the pendulum in upright position â€” while minimising the input voltage â€” we define the reward to be a weighted sum of $\\theta^2$, $\\dot{\\theta^2}$ and $u^2$.\n",
    "\n",
    "We will elaborate a bit more on this step function.\n",
    "The keys of observations and dictionaries correspond to respectively the value of the `observation` and `action` argument provided in the [connect method](https://eagerx.readthedocs.io/en/master/guide/api_reference/graph/graph.html?highlight=connect#eagerx.core.graph.Graph.connect).\n",
    "Note that inside the step method we call `self._step(action)` to get the observations.\n",
    "\n",
    "The [reset()](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.BaseEnv.reset) should also be defined by the user.\n",
    "The reset function allows to specify how states are reset at the beginning of an episode.\n",
    "Remember that we have one object (*Pendulum*) with one state (*model_state*).\n",
    "This *model_state* corresponds to $\\mathbf{x} = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}$.\n",
    "\n",
    "We will define the reset function such that it will reset the pendulum at the beginning of an episode to $\\mathbf{x} = \\begin{bmatrix} \\pi \\\\ 0 \\end{bmatrix}$.\n",
    "This corresponds to the downward position with zero velocity.\n",
    "Note that we call `self._reset(states)` to perform the actual reset and to obtain the new observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "306e54b7-61f9-454f-9243-1516f708519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PendulumEnv(eagerx.BaseEnv):\n",
    "    def __init__(self, name: str, rate: float, graph: eagerx.Graph, engine: eagerx.specs.EngineSpec):\n",
    "        \"\"\"Initializes an environment with EAGERx dynamics.\n",
    "\n",
    "        :param name: The name of the environment. Everything related to this environment\n",
    "                     (parameters, topics, nodes, etc...) will be registered under namespace: \"/[name]\".\n",
    "        :param rate: The rate (Hz) at which the environment will run.\n",
    "        :param graph: The graph consisting of nodes and objects that describe the environment's dynamics.\n",
    "        :param engine: The physics engine that will govern the environment's dynamics.\n",
    "        \"\"\"\n",
    "        # Make the backend specification\n",
    "        from eagerx.backends.single_process import SingleProcess\n",
    "        backend = SingleProcess.make()\n",
    "        \n",
    "        self.eval = eval\n",
    "        \n",
    "        # Maximum episode length\n",
    "        self.max_steps = 100\n",
    "        \n",
    "        # Step counter\n",
    "        self.steps = None\n",
    "        super().__init__(name, rate, graph, engine, backend, force_start=True)\n",
    "    \n",
    "    def step(self, action: Dict):\n",
    "        \"\"\"A method that runs one timestep of the environment's dynamics.\n",
    "\n",
    "        :params action: A dictionary of actions provided by the agent.\n",
    "        :returns: A tuple (observation, reward, done, info).\n",
    "\n",
    "            - observation: Dictionary of observations of the current timestep.\n",
    "\n",
    "            - reward: amount of reward returned after previous action\n",
    "\n",
    "            - done: whether the episode has ended, in which case further step() calls will return undefined results\n",
    "\n",
    "            - info: contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "        \"\"\"\n",
    "        # Take step\n",
    "        observation = self._step(action)\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Get angle \n",
    "        th = observation[\"angle\"][-1]\n",
    "\n",
    "        # START EXERCISE 1.5\n",
    "        # Update the step() method such that we use an estimate of dtheta to calculate the reward.\n",
    "        # Hint: you could use the previous_observation for this or observation[\"angle\"][-2].\n",
    "        # Hint: rate = 30 Hz\n",
    "        # thdot = observation[\"angular_velocity\"][-1]\n",
    "        thdot = 30 * (th - observation[\"angle\"][-2])  # Solution\n",
    "        # END EXERCISE 1.5\n",
    "\n",
    "        # Convert from numpy array to float\n",
    "        u = float(action[\"voltage\"])\n",
    "\n",
    "        # Normalize angle so it lies in [-pi, pi]\n",
    "        th -= 2 * np.pi * np.floor((th + np.pi) / (2 * np.pi))\n",
    "\n",
    "        # Calculate cost\n",
    "        # Penalize angle error, angular velocity and input voltage\n",
    "        cost = th**2 + 0.1 * thdot**2 + 0.001 * u**2\n",
    "\n",
    "        # Determine when is the episode over\n",
    "        # currently just a timeout after 100 steps\n",
    "        done = self.steps > self.max_steps\n",
    "\n",
    "        # Set info, tell the algorithm the termination was due to a timeout\n",
    "        # (the episode was truncated)\n",
    "        info = {\"TimeLimit.truncated\": self.steps > self.max_steps}\n",
    "        \n",
    "        return observation, -cost, done, info\n",
    "    \n",
    "    def reset(self) -> Dict:\n",
    "        \"\"\"Resets the environment to an initial state and returns an initial observation.\n",
    "\n",
    "        :returns: The initial observation.\n",
    "        \"\"\"\n",
    "        # Sample random reset states\n",
    "        states = self.state_space.sample()\n",
    "            \n",
    "        # Perform reset\n",
    "        observation = self._reset(states)\n",
    "\n",
    "        # Reset step counter\n",
    "        self.steps = 0\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da4afe-7163-4540-81a5-5793bf6225c5",
   "metadata": {},
   "source": [
    "We then initialize the environment a print the `action_space` and `observation_space`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5cc3a51-9abb-4329-896a-ca231f454209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[WARN]: Backend 'SINGLE_PROCESS' does not support multiprocessing, so all nodes are launched in the ENVIRONMENT process.\u001b[0m\n",
      "action_space:  Dict(voltage:Space([-2.], [2.], (1,), float32))\n",
      "observation_space:  Dict(angle:Box([-999. -999. -999.], [999. 999. 999.], (3,), float32), voltage_applied:Box([[-2.]], [[2.]], (1, 1), float32))\n"
     ]
    }
   ],
   "source": [
    "# Initialize Environment\n",
    "env = PendulumEnv(name=\"PendulumEnv\", rate=rate, graph=graph, engine=engine)\n",
    "\n",
    "# Print action & observation space\n",
    "print(\"action_space: \", env.action_space)\n",
    "print(\"observation_space: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fcbfde-f5e2-44dc-bf15-d796df4bc43e",
   "metadata": {},
   "source": [
    "Finally, we train the agent using [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/), similar to the first tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf343ae-0cb1-4726-b03d-f80830e61784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -995     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 404      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.9     |\n",
      "|    critic_loss     | 5.72     |\n",
      "|    ent_coef        | 0.916    |\n",
      "|    ent_coef_loss   | -0.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 303      |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 101       |\n",
      "|    ep_rew_mean     | -1.04e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 77        |\n",
      "|    time_elapsed    | 10        |\n",
      "|    total_timesteps | 808       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 30.9      |\n",
      "|    critic_loss     | 27.8      |\n",
      "|    ent_coef        | 0.814     |\n",
      "|    ent_coef_loss   | -0.287    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 707       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 101       |\n",
      "|    ep_rew_mean     | -1.04e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 75        |\n",
      "|    time_elapsed    | 16        |\n",
      "|    total_timesteps | 1212      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 49.1      |\n",
      "|    critic_loss     | 25.2      |\n",
      "|    ent_coef        | 0.73      |\n",
      "|    ent_coef_loss   | -0.373    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1111      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 101       |\n",
      "|    ep_rew_mean     | -1.06e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 74        |\n",
      "|    time_elapsed    | 21        |\n",
      "|    total_timesteps | 1616      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 68.1      |\n",
      "|    critic_loss     | 21.9      |\n",
      "|    ent_coef        | 0.659     |\n",
      "|    ent_coef_loss   | -0.466    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1515      |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from eagerx.wrappers import Flatten\n",
    "from gym.wrappers.rescale_action import RescaleAction\n",
    "\n",
    "# Stable Baselines3 expects flattened actions & observations\n",
    "# Convert observation and action space from Dict() to Box(), normalize actions\n",
    "env = Flatten(env)\n",
    "env = RescaleAction(env, min_action=-1.0, max_action=1.0)\n",
    "\n",
    "# Check that env follows Gym API and returns expected shapes\n",
    "check_env(env)\n",
    "\n",
    "# Toggle render\n",
    "env.render(\"human\")\n",
    "\n",
    "# Initialize learner\n",
    "model = sb3.SAC(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train for 1 minute (sim time)\n",
    "model.learn(total_timesteps=int(60 * rate))\n",
    "\n",
    "env.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd13fcd-7275-40a6-b779-3caef149dd55",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercises\n",
    "\n",
    "In these exercises we will address the scenario in which the angular velocity $\\dot{\\theta}$ is not available.\n",
    "So now we want to learn to swing up the pendulum using the angle $\\theta$ only.\n",
    "Step-by-step we will guide you through the process of creating an environment for this scenario in the following exercises.\n",
    "\n",
    "For these exercises, you will need to modify or add some lines of code in the cells above.\n",
    "These lines are indicated by the following comments:\n",
    "\n",
    "```python\n",
    "# START EXERCISE [BLOCK_NUMBER]\n",
    "\n",
    "# END EXERCISE [BLOCK_NUMBER]\n",
    "```\n",
    "\n",
    "However, feel free to play with the other code as well if you are interested.\n",
    "We recommend you to restart and run all code after each section (in Colab there is the option *Restart and run all* under *Runtime*).\n",
    "\n",
    "\n",
    "## 1. Violation of the Markov property\n",
    "\n",
    "We could naively remove the sensor *dtheta* in the environment above and train the agent.\n",
    "However, this will most likely not result in a successful policy because the [Markov property](http://www.incompleteideas.net/book/3/node6.html) is violated.\n",
    "It is not possible to fully restore the Markov property without observing $\\dot{\\theta}$, but we can create a representation that is sufficient for solving the task.\n",
    "If we stack the last three measurements of $\\theta$ and provide this information as an observation, the agent will be able to approximate $\\dot{\\theta}$ (e.g. using a finite difference method).\n",
    "With this information, the agent can estimate the angular velocity at the previous time step.\n",
    "If we also provide the last applied action as an observation, the agent will be able to estimate $\\dot{\\theta}$ at the current time step.\n",
    "\n",
    "After this the graph should look as follows:\n",
    "\n",
    "<img src=\"./figures/tutorial_21_gui.svg\" width=720>\n",
    "\n",
    "Furthermore, the Markov property can also be violated due to delays.\n",
    "If we want our policy to transfer from simulation to a real system, we also need to account for delays that are present in the real world.\n",
    "Therefore we will simulate that the sensor *theta* has a delay of 0.01 seconds.\n",
    "\n",
    "We also have to update the `step()` method, since we no longer have the *angular_velocity* observation.\n",
    "In the reward function, we still want to penalize the angular velocity.\n",
    "Therefore we will have to approximate $\\dot{\\theta}$ in `step()`, which could for example be done as follows: $\\hat{\\dot{\\theta}} = \\text{rate} \\times (\\theta_k  - \\theta_{k - 1})$ where $k$ is the time step and rate is the `rate` of the [environment](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html) in Hz.\n",
    "\n",
    "### Add your code to the following blocks: \n",
    "\n",
    "1.1 Remove sensor *dtheta* from and add sensor *u_applied* to the list of sensors.  \n",
    "1.2 Remove the connection from sensor *dtheta*.  \n",
    "1.3 Also, modify the connection with source *theta* and set `window` = 3 to stack the last three observations of $\\theta$ and set `delay` to 0.01.  \n",
    "1.4 Connect *u_applied* to an observation called *voltage_applied* with `window` = 1. Do you know why *u_applied* should be an observation to the agent in order to restore the Markov property?  \n",
    "1.5 Update the`step()` method such that we use an estimate of $\\dot{\\theta}$ to calculate the reward.\n",
    "\n",
    "## 2. Initial state sampling and domain randomization\n",
    "\n",
    "Next, we will add domain randomization [domain randomization](https://sites.google.com/view/domainrandomization/), in order to improve the robustness against model inacurracies.\n",
    "If we want to transfer a policy from simulation to a real system, we need to be aware that the model used for simulation is inaccurate and that the agent could possibly exploit these inaccuracies.\n",
    "One of the techniques for addressing this problem is domain randomization, i.e. varying over simulator parameters in order to improve the robustness of the resulting policy.\n",
    "More specifically, we will do this by varying over the ODE parameters ($m, l, J, b, K$ and $R$).\n",
    "We can do this by adding the *model_parameters* state.\n",
    "\n",
    "After this the graph should look as follows:\n",
    "\n",
    "<img src=\"./figures/tutorial_22_gui.svg\" width=720>\n",
    "\n",
    "We will also improve the reset procedure of the environment.\n",
    "At the beginning of each episode, the environment is reset.\n",
    "In the code as provided above, the pendulum is reset to the downward position with zero velocity each episode.\n",
    "However, the initial state distribution can have a significant influence on the learning speed.\n",
    "If we sample the $\\mathbf{x}_0 = \\begin{bmatrix} \\pi \\\\ 0 \\end{bmatrix}$ initial state every time, it will take many timesteps for the agent to obtain experience for $-\\frac{\\pi}{2} < \\theta < \\frac{\\pi}{2}$.\n",
    "Namely, in the beginning the policy will be random and it is unlikely that acting randomly will result in the pendulum gaining enough momentum to move upwards.\n",
    "This is problematic, since the agent will obtain the highest rewards when the pendulum is pointed upwards.\n",
    "If the agent does not explore enough (see [the exploration-exploitation trade-off](http://www.incompleteideas.net/book/2/node2.html)), the agent will not know that it can obtain the highest rewards by swinging the pendulum upward.\n",
    "Therefore, we will update the `reset()` method, such that we sample the initial state randomly, rather than sampling $\\mathbf{x}_0 = \\begin{bmatrix} \\pi \\\\ 0 \\end{bmatrix}$ everytime.\n",
    "We also need to make sure that the aforementioned *model_parameters* state that is reset to perform domain randomization.\n",
    "\n",
    "### Add your code to the following blocks: \n",
    "\n",
    "2.1 Add the state *model_parameters* to the list of states of the pendulum.  Now,  both the `model_state` and `model_parameters` states are reset to random values at the beginning of each episode. This happens in the `env.reset()` function on line:\n",
    "```python\n",
    "states: Dict = self.state_space.sample()\n",
    "```\n",
    "You can also sample individual states from the environment's state space as follows:\n",
    "```python\n",
    "state: np.ndarray = env.state_space[\"<object_name>/<state_name>\"].sample()\n",
    "```\n",
    "where `object_name` should be replaced with the name of the object (e.g. *pendulum*) and `state_name` with the name of the state (e.g. *model_parameters*).\n",
    "\n",
    "### Note\n",
    "After implementing these modifications, the pendulum is still not swinging up... In the next tutorial you will find out why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d030cbd-acc0-4b53-8af4-1fd8e853646a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
