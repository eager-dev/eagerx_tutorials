{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5c1648-6268-4829-bbaf-53ef779bd1a0",
   "metadata": {},
   "source": [
    "# Tutorial 8: Reset Routines\n",
    "\n",
    "Resetting an object to a desired state is non-trivial in the real-world compared to simulation, where resets can often be caried out with a single command. In this tutorial, we will discuss methods to include reset routines into your graph that can reset an object's state that work both in simulation and the real-world. \n",
    "\n",
    "The following will be covered:\n",
    "<!-- - Defining an object's state with an [`EngineState`](https://eagerx.readthedocs.io/en/master/guide/api_reference/engine_state/index.html).  -->\n",
    "- Defining the reset routine with a [`ResetNode`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html).\n",
    "- Reset the object's state with the [`ResetNode`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html).\n",
    "\n",
    "In the remainder of this tutorial, we will go more into detail on this concept.\n",
    "\n",
    "Furthermore, at the end of this notebook you will find an exercise.\n",
    "For the exercise you will have to add/modify a couple of lines of code, which are marked by\n",
    "\n",
    "```python\n",
    "\n",
    "# START EXERCISE [BLOCK_NUMBER]\n",
    "\n",
    "# END EXERCISE [BLOCK_NUMBER]\n",
    "```\n",
    "\n",
    "## Pendulum Swing-up\n",
    "\n",
    "We will assume that we already have the object definition of the underactuated pendulum that we used in the [first](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/1_environment_creation.ipynb) tutorial with its dynamics simulated with the [OdeEngine](https://github.com/eager-dev/eagerx_ode). \n",
    "\n",
    "Our goal is to create a [`ResetNode`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html) that can reset the pendulum to a desired state (i.e. $\\theta=\\theta_\\text{des}$ and $\\dot{\\theta}=0$) without requiring a simulator reset. In other words, the [`ResetNode`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html) will receive the desired state as a target and it will send actuator commands until the pendulum has reached this state.\n",
    "\n",
    "## Activate GPU (Colab only)\n",
    "\n",
    "When in Colab, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "## Notebook Setup\n",
    "\n",
    "In order to be able to run the code, we need to install the *eagerx_tutorials* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8b6d03-40d4-4b45-92b2-61ffa09a3249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab.\n",
      "Installing eagerx-gui\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import eagerx_tutorials\n",
    "except ImportError:\n",
    "    !{\"echo 'Installing eagerx-tutorials with pip.' && pip install eagerx-tutorials >> /tmp/eagerx_install.txt 2>&1\"}\n",
    "\n",
    "# Setup interactive notebook\n",
    "# Required in interactive notebooks only.\n",
    "from eagerx_tutorials import helper\n",
    "helper.setup_notebook()\n",
    "\n",
    "# Import eagerx\n",
    "import eagerx\n",
    "eagerx.set_log_level(eagerx.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff0b7a-16a8-4628-9fe0-0f16e3a3804a",
   "metadata": {},
   "source": [
    "## How do [ResetNodes](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html) work?\n",
    "\n",
    "As mentioned before, resetting an object is non-trivial in the real-world compared to simulation, where resets can often be caried out with a single command. We developed [`ResetNodes`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html) in EAGERx to allow users to easily define reset routines that may, for example use pre-defined controllers, to reset an Object to a desired state.\n",
    "\n",
    "The structure of a [`ResetNode`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html) is very similar to a conventional [`Node`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/node.html). However, the [`callback()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html#eagerx.core.entities.ResetNode.callback) of a [`ResetNode`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html) is skipped until the agent/user calls [`.reset()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv.reset) on the gym evironment. At that moment, the desired state that that was selected in the [reset function](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv.reset_fn) (convered in [this tutorial](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/2_reset_and_step.ipynb)) is send to the [`ResetNode`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html) as a `target` state.\n",
    "\n",
    "The [`ResetNode`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html) takes over control and starts sending commands to the object's actuators until the object's current state is equal (or close to) that target state. In other words, after the agent/user calls [`.reset()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv.reset) the [`ResetNode.callback()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html#eagerx.core.entities.ResetNode.callback) is called at the specified node `rate` with the connected `inputs` together with the `target` state and will produce `outputs` that bring the object's state closer to the desired state. During each callback, the [`ResetNode`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html) assess the status of the reset routine (i.e. whether the `target` state was reached) and communicates this status to the engine with a message.    \n",
    "\n",
    "**Important**: To assure input-output synchronization in [`sync`](https://eagerx.readthedocs.io/en/master/guide/api_reference/engine/index.html#eagerx.core.entities.Engine.sync) mode, the [`ResetNode`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html) must be placed in-between the actions commanded by the agent/user and the object actuator. Over the course of an episode, the reset node simply feeds through all commands, and only after [`.reset()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv.reset) is called, will the commands be produced by [`ResetNode.callback()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html#eagerx.core.entities.ResetNode.callback). The reset node's `rate` is constrained to be equal to the rate of the commands that it must feedthrough. \n",
    "## How to define a [ResetNode](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html)?\n",
    "\n",
    "We can create a reset node by inheriting from the class [`ResetNode`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html).\n",
    "This class has the following abstract methods we need to implement:\n",
    "\n",
    "- [`make()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html#eagerx.core.entities.ResetNode.make): Makes the parameter specification of the node.\n",
    "- [`initialize()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html#eagerx.core.entities.ResetNode.initialize): Initializes the node.\n",
    "- [`reset()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html#eagerx.core.entities.ResetNode.reset): Resets the node at the beginning of an episode.\n",
    "- [`callback()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html#eagerx.core.entities.ResetNode.callback): Called at the rate of the node after the agent/user calls [`.reset()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv.reset) on the gym evironment. It receives all connected `inputs` and `targets` as arguments.\n",
    "\n",
    "\n",
    "## An example [ResetNode](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html)\n",
    "\n",
    "To illustrate how [`ResetNodes`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html) work, we will again create an environment with the *Pendulum* object, like we did in the [first](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/1_environment_creation.ipynb) and [second](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/2_reset_and_step.ipynb) tutorials. We will add a [`ResetNode`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html) called `angle_reset` and connect it as illustrated in the graph below:\n",
    "\n",
    "<img src=\"../figures/tutorial_7_gui.svg\" width=720>\n",
    "\n",
    "- We connect the actions commanded by the agent (i.e. `voltage`) to the `feedthrough` connection `u` (light blue color) and we connect the `angle_reset`'s output `u` to the pendulum's actuator `u`. In this way, the `voltage` actions will be fed through to the output `u` during an episode, while the `angle_reset`'s [`callback()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html#eagerx.core.entities.ResetNode.callback) will produce the outputs during a reset.\n",
    "- We connect the pendulum's `model_state` to the `angle_reset`'s target `goal`. In this way, the `angle_reset`'s [`callback()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/reset_node.html#eagerx.core.entities.ResetNode.callback) will receive the desired `model_state` as an argument.\n",
    "- To assess the status of the reset routine (i.e. whether the `model_state` state was reached, we connect the two pendulum sensors `theta` and `theta_dot` as inputs to `angle_reset`.\n",
    "\n",
    "Below, the definition of this reset node is given. Currently, the node uses a [PID](https://en.wikipedia.org/wiki/PID_controller) to reset the pendulum to a angle with zero angular velocity. If the reset takes too long, we timeout and consider the reset finished regardless of whether the target state was reached.\n",
    "\n",
    "In the exercise of this tutorial, we will modify this routine to, instead, apply random actions for a fixed amount of time (disregarding the desired state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878e4395-cb31-40ed-9974-b4b82ae0c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from eagerx import Space, ResetNode\n",
    "from eagerx.core.specs import ResetNodeSpec\n",
    "from eagerx.utils.utils import Msg\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def wrap_angle(angle):\n",
    "    return angle - 2 * np.pi * np.floor((angle + np.pi) / (2 * np.pi))\n",
    "\n",
    "\n",
    "class ResetAngle(ResetNode):\n",
    "    @classmethod\n",
    "    def make(\n",
    "        cls,\n",
    "        name: str,\n",
    "        rate: float,\n",
    "        threshold: float = 0.1,\n",
    "        timeout: float = 5.0,\n",
    "        gains: Optional[List[float]] = None,\n",
    "        u_range: Optional[List[float]] = None,\n",
    "    ) -> ResetNodeSpec:\n",
    "        \"\"\"This AngleReset node resets the pendulum to a desired angle with zero angular velocity. Note that this controller\n",
    "        only works properly when resetting the pendulum near the downward facing equilibrium.\n",
    "\n",
    "        :param name: Node name\n",
    "        :param rate: Rate at which callback is called. Must be equal to the rate of the nodes that are connect to the feedthroughs.\n",
    "        :param threshold: Absolute difference between the desired and goal state before considering the reset complete.\n",
    "        :param timeout: Maximum time (seconds) before considering the reset finished (regardless whether the goal was reached).\n",
    "        :param gains: Gains of the PID controller used to reset.\n",
    "        :param u_range: Min and max action.\n",
    "        :return: Specification.\n",
    "        \"\"\"\n",
    "        # Get base parameter specification with defaults parameters\n",
    "        spec = cls.get_specification()\n",
    "\n",
    "        # Modify default node params\n",
    "        spec.config.update(name=name, rate=rate, process=eagerx.process.ENVIRONMENT, color=\"grey\")\n",
    "        spec.config.update(inputs=[\"theta\", \"theta_dot\"], targets=[\"goal\"], outputs=[\"u\"])\n",
    "        spec.config.update(u_range=u_range, threshold=threshold, timeout=timeout)\n",
    "        # Proportional (Kp), derivative (Kd) and integral (Ki) gains\n",
    "        spec.config.gains = gains if isinstance(gains, list) else [1.0, 0.5, 0.0]\n",
    "\n",
    "        # Add space_converter\n",
    "        c = Space(low=[u_range[0]], high=[u_range[1]], dtype=\"float32\")\n",
    "        spec.outputs.u.space = c\n",
    "        return spec\n",
    "\n",
    "    def initialize(self, spec: ResetNodeSpec):\n",
    "        self.threshold = spec.config.threshold\n",
    "        self.timeout = spec.config.timeout\n",
    "        self.u_min, self.u_max = spec.config.u_range\n",
    "        \n",
    "        # Creat a simple PID controller\n",
    "        from eagerx_tutorials.pendulum.pid import PID\n",
    "        gains = spec.config.gains\n",
    "        self.controller = PID(u0=0.0, kp=gains[0], kd=gains[1], ki=gains[2], dt=1 / self.rate)\n",
    "\n",
    "    @eagerx.register.states()\n",
    "    def reset(self):\n",
    "        # Reset the internal state of the PID controller (ie the error term).\n",
    "        self.controller.reset()\n",
    "        self.ts_start_routine = None\n",
    "\n",
    "    @eagerx.register.inputs(theta=Space(shape=(), dtype=\"float32\"), \n",
    "                            theta_dot=Space(shape=(), dtype=\"float32\"))\n",
    "    @eagerx.register.targets(goal=Space(low=[-3.14, -9.0], high=[3.14, 9.0], dtype=\"float32\"))\n",
    "    @eagerx.register.outputs(u=Space(dtype=\"float32\"))\n",
    "    def callback(self, t_n: float, goal: Msg, theta: Msg, theta_dot: Msg):\n",
    "        if self.ts_start_routine is None:\n",
    "            self.ts_start_routine = t_n\n",
    "\n",
    "        # Convert messages to floats and numpy array\n",
    "        theta = theta.msgs[-1]  # Take the last received message\n",
    "        theta_dot = theta_dot.msgs[-1]  # Take the last received message\n",
    "        goal = np.array(goal.msgs[-1], dtype=\"float32\")  # Take the last received message\n",
    "\n",
    "        # Define downward angle as theta=0 (resolve downward discontinuity)\n",
    "        theta += np.pi\n",
    "        goal[0] += np.pi\n",
    "\n",
    "        # Wrap angle between [-pi, pi]\n",
    "        theta = wrap_angle(theta)\n",
    "        goal[0] = wrap_angle(goal[0])\n",
    "\n",
    "        # Overwrite the desired velocity to be zero.\n",
    "        goal[1] = 0.0\n",
    "\n",
    "        # Calculate the action using the PID controller\n",
    "        # START EXERCISE 1.2\n",
    "        # Select random actions instead.\n",
    "        # u = self.controller.next_action(theta, ref=goal[0])\n",
    "        u = np.random.uniform(low=self.u_min, high=self.u_max)  # Solution\n",
    "        \n",
    "        # PID: Determine If we have reached our goal state\n",
    "        # Random Actions: We timeout if the routine takes too long and simply assume that we are done.\n",
    "        # done = np.isclose(np.array([theta, theta_dot]), goal, atol=self.threshold).all().item()\n",
    "        done = (t_n - self.ts_start_routine) > self.timeout  # Solution\n",
    "        # END EXERCISE 1.2\n",
    "        \n",
    "        # Clip actions\n",
    "        u = np.clip(u, self.u_min, self.u_max)  # Clip u to range\n",
    "        \n",
    "        # Prepare output message for transmission.\n",
    "        # This must contain a message for every registered & selected output and target.\n",
    "        # For targets, this message decides whether the goal state has been reached (or we, for example, timeout the reset).\n",
    "        # The name for this target message is the registered target name + \"/done\".\n",
    "        output_msgs = {\"u\": np.array([u], dtype=\"float32\"), \"goal/done\": done}\n",
    "        return output_msgs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e352c4-b765-4f3b-b870-358abe3b3916",
   "metadata": {},
   "source": [
    "After defining & registering the reset node above, we can create it and add it to the graph. We will then proceed to connect it according to the GUI visualization of the intended graph shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858c3f13-6c63-4f92-88d3-872220769fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define rate in Hz\n",
    "rate = 30.0\n",
    "\n",
    "# Initialize empty graph\n",
    "graph = eagerx.Graph.create()\n",
    "\n",
    "# Create a pendulum\n",
    "from eagerx_tutorials.pendulum.objects import Pendulum\n",
    "pendulum = Pendulum.make(\"pendulum\", actuators=[\"u\"], sensors=[\"theta\", \"theta_dot\", \"image\"], states=[\"model_state\"])\n",
    "\n",
    "# Add pendulum to the graph\n",
    "graph.add(pendulum)\n",
    "\n",
    "# Connect the pendulum to an action and observation\n",
    "graph.connect(source=pendulum.sensors.theta, observation=\"angle\")\n",
    "graph.connect(source=pendulum.sensors.theta_dot, observation=\"angular_velocity\")\n",
    "\n",
    "# Create the reset node\n",
    "u_min = pendulum.actuators.u.space.low\n",
    "u_max = pendulum.actuators.u.space.high\n",
    "reset = ResetAngle.make(\"reset_angle\", rate, gains=[2.0, 0.2, 1.0], u_range=[u_min, u_max])\n",
    "\n",
    "# Add the reset node to the graph\n",
    "graph.add(reset)\n",
    "\n",
    "# Connect the pendulum state as the reset's target.\n",
    "graph.connect(source=pendulum.states.model_state, target=reset.targets.goal)\n",
    "\n",
    "# Connect the action we are feeding through during the course of an episode, but will be produced by the reset node during a reset.\n",
    "# During normal operations, the ResetNode simply feeds through the voltage actionto reset.outputs.u.\n",
    "graph.connect(action=\"voltage\", target=reset.feedthroughs.u)\n",
    "\n",
    "# When env.reset() is called, no voltage actions are being send by the agent, because we are resetting.\n",
    "# At that moment, the ResetNode's callback will be called instead to produce the voltages. \n",
    "graph.connect(source=reset.outputs.u, target=pendulum.actuators.u)\n",
    "\n",
    "# To decide on the voltage actions that will bring the current pendulum state closer to the desired target state,\n",
    "# The ResetNode requires knowledge of the current pendulum angle information. Hence, we connect them as inputs.\n",
    "# These inputs are also used by the reset node to determine whether the target state has been reached.\n",
    "# If so, the reset node signals EagerxEnv that the desired state was reached.\n",
    "graph.connect(source=pendulum.sensors.theta, target=reset.inputs.theta)\n",
    "graph.connect(source=pendulum.sensors.theta_dot, target=reset.inputs.theta_dot)\n",
    "\n",
    "# Define the render source\n",
    "graph.render(source=pendulum.sensors.image, rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a4dd8-9a01-47fc-bbf8-b24a6e716a8e",
   "metadata": {},
   "source": [
    "Next, we will define a reset function for the environment that selects desired states. Unfortunately, we cannot sample any angle because the PID controller won't be able to reset to angles near the upright position due to the underactuation. Hence, we will start by always selecting the downward position of the pendulum. \n",
    "\n",
    "In the exercise, we will selecte angles that are sampled around the downward position of the pendulum to improve state-space coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "912cfd51-e9a6-48ff-82f0-dbee89123bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PendulumEnv(eagerx.BaseEnv):\n",
    "    def __init__(self, name: str, rate: float, graph: eagerx.Graph, engine: eagerx.Engine, render_mode=\"human\"):\n",
    "        \"\"\"Initializes an environment with EAGERx dynamics.\n",
    "\n",
    "        :param name: The name of the environment. Everything related to this environment\n",
    "                     (parameters, topics, nodes, etc...) will be registered under namespace: \"/[name]\".\n",
    "        :param rate: The rate (Hz) at which the environment will run.\n",
    "        :param graph: The graph consisting of nodes and objects that describe the environment's dynamics.\n",
    "        :param engine: The physics engine that will govern the environment's dynamics.\n",
    "        :param render_mode: Defines the render mode (e.g. \"human\", \"rgb_array\").\n",
    "        \"\"\"\n",
    "        # Make the backend specification\n",
    "        from eagerx.backends.single_process import SingleProcess\n",
    "        backend = SingleProcess.make()\n",
    "        \n",
    "        self.eval = eval\n",
    "        \n",
    "        # Maximum episode length\n",
    "        self.max_steps = 100\n",
    "        \n",
    "        # Step counter\n",
    "        self.steps = None\n",
    "        super().__init__(name, rate, graph, engine, backend, force_start=True, render_mode=render_mode)\n",
    "    \n",
    "    def step(self, action: Dict):\n",
    "        \"\"\"A method that runs one timestep of the environment's dynamics.\n",
    "\n",
    "        :params action: A dictionary of actions provided by the agent.\n",
    "        :returns: A tuple (observation, reward, truncated, done, info).\n",
    "\n",
    "            - observation: Dictionary of observations of the current timestep.\n",
    "\n",
    "            - reward: amount of reward returned after previous action\n",
    "            \n",
    "            - truncated: Whether the episode ended with a timeout\n",
    "            \n",
    "            - done: whether the episode has ended, in which case further step() calls will return undefined results\n",
    "\n",
    "            - info: contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "        \"\"\"\n",
    "        # Take step\n",
    "        observation = self._step(action)\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Get angle and angular velocity\n",
    "        # Take first element because of window size (covered in other tutorial)\n",
    "        th = observation[\"angle\"][0]\n",
    "        thdot = observation[\"angular_velocity\"][0]\n",
    "\n",
    "        # Convert from numpy array to float\n",
    "        u = float(action[\"voltage\"])\n",
    "\n",
    "        # Calculate cost\n",
    "        # Penalize angle error, angular velocity and input voltage\n",
    "        cost = th**2 + 0.1 * (thdot / (1 + 10 * abs(th))) ** 2 + 0.01 * u ** 2  \n",
    "\n",
    "        # Determine when is the episode over\n",
    "        # currently just a timeout after 100 steps\n",
    "        done = self.steps > self.max_steps\n",
    "        truncated = self.steps > self.max_steps\n",
    "\n",
    "        # Set info, tell the algorithm the termination was due to a timeout\n",
    "        # (the episode was truncated)\n",
    "        info = {\"TimeLimit.truncated\": self.steps > self.max_steps}\n",
    "        \n",
    "        # Render\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return observation, -cost, truncated, done, info\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Resets the environment to an initial state and returns an initial observation.\n",
    "\n",
    "        :returns: The initial observation.\n",
    "        \"\"\"\n",
    "        # Determine reset states\n",
    "        states = self.state_space.sample()\n",
    "        \n",
    "        # START EXERCISE 1.1\n",
    "        # Sample angles near the downward position.\n",
    "        # Hint: angles are in [-pi, pi] and upright is theta=0.\n",
    "        # states[\"pendulum/model_state\"] = np.array([3.14, 0], dtype=\"float32\")\n",
    "        theta = 3.14 * np.random.uniform(low=0.75, high=1.0) * [-1,1][np.random.randint(2)]  # Solution\n",
    "        states[\"pendulum/model_state\"][:] = [theta, 0]  # Solution\n",
    "        \n",
    "        # END EXERCISE 1.1\n",
    "\n",
    "        # Perform reset\n",
    "        observation = self._reset(states)\n",
    "        info = {}\n",
    "\n",
    "        # Reset step counter\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Render\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return observation, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405eb3cb-6675-429d-aa9f-77ffa21684c0",
   "metadata": {},
   "source": [
    "We will proceed with defining the engine and initializing the environment. \n",
    "\n",
    "Finally, we train the agent using [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/), again similar to the preceding tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d2bf3-8a66-4605-a31f-e785e0a17b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[WARN]: Backend 'SINGLE_PROCESS' does not support multiprocessing, so all nodes are launched in the ENVIRONMENT process.\u001b[0m\n",
      "action_space:  Dict('voltage': Space([-2.], [2.], (1,), float32))\n",
      "observation_space:  Dict('angle': Box(-999.0, 999.0, (1,), float32), 'angular_velocity': Box(-999.0, 999.0, (1,), float32))\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -978     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 48       |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 404      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 0.619    |\n",
      "|    ent_coef        | 0.914    |\n",
      "|    ent_coef_loss   | -0.135   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 303      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -939     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 49       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 808      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32.8     |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.819    |\n",
      "|    ent_coef_loss   | -0.293   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 707      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -911     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 49       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 1212     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.6     |\n",
      "|    critic_loss     | 19.3     |\n",
      "|    ent_coef        | 0.746    |\n",
      "|    ent_coef_loss   | -0.248   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1111     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -890     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 49       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 1616     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 59.6     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.693    |\n",
      "|    ent_coef_loss   | -0.211   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1515     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -875     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 49       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 2020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 71.7     |\n",
      "|    critic_loss     | 3.26     |\n",
      "|    ent_coef        | 0.654    |\n",
      "|    ent_coef_loss   | -0.157   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1919     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -862     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 48       |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 2424     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 84.1     |\n",
      "|    critic_loss     | 4.03     |\n",
      "|    ent_coef        | 0.624    |\n",
      "|    ent_coef_loss   | -0.0774  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2323     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -852     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 48       |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 2828     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 95.6     |\n",
      "|    critic_loss     | 36.4     |\n",
      "|    ent_coef        | 0.598    |\n",
      "|    ent_coef_loss   | -0.0426  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2727     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from eagerx.wrappers import Flatten\n",
    "\n",
    "# Make the engine\n",
    "from eagerx_ode.engine import OdeEngine\n",
    "engine = OdeEngine.make(rate=rate)\n",
    "\n",
    "# Initialize Environment\n",
    "env = PendulumEnv(name=\"PendulumEnv\", rate=rate, graph=graph, engine=engine)\n",
    "\n",
    "# Print action & observation space\n",
    "print(\"action_space: \", env.action_space)\n",
    "print(\"observation_space: \", env.observation_space)\n",
    "\n",
    "# Stable Baselines3 expects flattened actions & observations\n",
    "# Convert observation and action space from Dict() to Box(), normalize actions\n",
    "env = Flatten(env)\n",
    "env = helper.RescaleAction(env, min_action=-1.0, max_action=1.0)\n",
    "\n",
    "# Check that env follows Gym API and returns expected shapes\n",
    "check_env(env)\n",
    "\n",
    "# Initialize learner\n",
    "model = sb3.SAC(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train for 1 minute (sim time)\n",
    "model.learn(total_timesteps=int(150 * rate))\n",
    "\n",
    "env.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872df1d1-bed3-4b09-80dd-44c8b71a502d",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In this exercise you will modify the reset routine defined above. \n",
    "\n",
    "For this exercise, you will need to modify or add some lines of code in the cells above.\n",
    "These lines are indicated by the following comments:\n",
    "\n",
    "```python\n",
    "# START EXERCISE [BLOCK_NUMBER]\n",
    "\n",
    "# END EXERCISE [BLOCK_NUMBER]\n",
    "```\n",
    "\n",
    "However, feel free to play with the other code as well if you are interested.\n",
    "We recommend you to restart and run all code after each section (in Colab there is the option *Restart and run all* under *Runtime*).\n",
    "\n",
    "## 1. Modify the reset procedure\n",
    "\n",
    "\n",
    "### Add your code to the following blocks: \n",
    "\n",
    "1.1 Change the `reset()` method of the environment, such that the desired angles are sampled randomly around the downward position of the pendulum.\n",
    "This will improve state-space coverage and improve the learning rate.  \n",
    "1.2 Next, modify the callback of the reset node such that we do not use the PID controller, but perform random actions for 2 seconds before considering the reset finished. \n",
    "This will improve state-space coverage even more, because we now also allow for non-zero angular velocity resets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
