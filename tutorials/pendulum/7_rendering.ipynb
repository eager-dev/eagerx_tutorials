{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5c1648-6268-4829-bbaf-53ef779bd1a0",
   "metadata": {},
   "source": [
    "# Tutorial 7: More Informative Rendering\n",
    "\n",
    "In this tutorial, we will discuss how to make rendering more informative. We will also demonstrate that rendering is agnostic to the selected physics-engine.\n",
    "\n",
    "The following will be covered:\n",
    "- Create an overlay node that augments a raw image sensors\n",
    "- Connect the overlay node and use it for rendering\n",
    "- Demonstrate that rendering is agnostic to the selected physics-engine\n",
    "\n",
    "In the remainder of this tutorial, we will go more into detail on this concept.\n",
    "\n",
    "Furthermore, at the end of this notebook you will find an exercise.\n",
    "For the exercise you will have to add/modify a couple of lines of code, which are marked by\n",
    "\n",
    "```python\n",
    "\n",
    "# START EXERCISE [BLOCK_NUMBER]\n",
    "\n",
    "# END EXERCISE [BLOCK_NUMBER]\n",
    "```\n",
    "\n",
    "## Pendulum Swing-up\n",
    "\n",
    "We will assume that we already have the object definition of the underactuated pendulum that we used in the [first](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/1_environment_creation.ipynb) tutorial with its dynamics simulated with the [OdeEngine](https://github.com/eager-dev/eagerx_ode). We will also assume that the *engine-specific* implementation we created in the [previous tutorial](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/5_engine_implementation.ipynb) for the [GymEngine](https://github.com/eager-dev/eagerx/blob/master/eagerx/engines/openai_gym/engine.py) is available.\n",
    "\n",
    "Our goal is to make the rendered images more informative. We will lay the actions, selected by the agent, over the raw images produced by the image sensor of the pendulum. We will then visualise the augmented images instead of the raw images from the image sensor.\n",
    "\n",
    "## Activate GPU (Colab only)\n",
    "\n",
    "When in Colab, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8b6d03-40d4-4b45-92b2-61ffa09a3249",
   "metadata": {
    "cellView": "form",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab.\n"
     ]
    }
   ],
   "source": [
    "#@title Notebook Setup\n",
    "\n",
    "#@markdown In order to be able to run the code, we need to install the *eagerx_tutorials* package.\n",
    "\n",
    "try:\n",
    "    import eagerx_tutorials\n",
    "except ImportError:\n",
    "    !{\"echo 'Installing setuptools with pip.' && pip install setuptools==65.5.0 wheel==0.38.4 >> /tmp/setuptools_install.txt 2>&1\"}\n",
    "    !{\"echo 'Installing eagerx-tutorials with pip.' && pip install eagerx-tutorials >> /tmp/eagerx_install.txt 2>&1\"}\n",
    "\n",
    "# Setup interactive notebook\n",
    "# Required in interactive notebooks only.\n",
    "from eagerx_tutorials import helper\n",
    "helper.setup_notebook()\n",
    "\n",
    "# Import eagerx\n",
    "import eagerx\n",
    "eagerx.set_log_level(eagerx.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff0b7a-16a8-4628-9fe0-0f16e3a3804a",
   "metadata": {},
   "source": [
    "## Let's get started\n",
    "\n",
    "We will again create an environment with the *Pendulum* object, like we did in the [first](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/1_environment_creation.ipynb) and [second](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/2_reset_and_step.ipynb) tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1fbe40-73a6-4135-b7dd-6cd42b52dd0c",
   "metadata": {
    "cellView": "form",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown Let's make the *Pendulum* object and add it to an empty graph.\n",
    "\n",
    "# Make the pendulum\n",
    "from eagerx_tutorials.pendulum.objects import Pendulum\n",
    "import eagerx_tutorials.pendulum.gym_implementation  # noqa: registers gym implementation\n",
    "pendulum = Pendulum.make(\"pendulum\", actuators=[\"u\"], sensors=[\"theta\", \"theta_dot\", \"image\"], states=[\"model_state\"])\n",
    "\n",
    "# Define rate in Hz\n",
    "rate = 30.0\n",
    "\n",
    "# Initialize empty graph\n",
    "graph = eagerx.Graph.create()\n",
    "\n",
    "# Add pendulum to the graph\n",
    "graph.add(pendulum)\n",
    "\n",
    "# Connect the pendulum to an action and observation\n",
    "graph.connect(action=\"voltage\", target=pendulum.actuators.u)\n",
    "graph.connect(source=pendulum.sensors.theta, observation=\"angle\")\n",
    "graph.connect(source=pendulum.sensors.theta_dot, observation=\"angular_velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8f9fd-5698-4a26-8fbc-a453881444ac",
   "metadata": {},
   "source": [
    "Below we have defined a overlay node. The callback receives two inputs, namely the applied action `u` and a `raw_image`. In the callback, the applied action is visualised as an overlay on top of the raw image. The resulting `image` is the output of the callback. If we render this output, instead of the raw `pendulum.sensors.image`, we get a more informative rendered image.\n",
    "\n",
    "In the exercise of this tutorial, we will finish the code that defines the Overlay node below. Specifically, we will overlay some text on top of the image that indicates the current timestamp.\n",
    "\n",
    "Similar to what we covered in [this](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/4_nodes.ipynb) tutorial, we can create this node by inheriting from the class [`Node`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/node.html).\n",
    "This class has the following abstract methods we need to implement:\n",
    "\n",
    "- [`make()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/node.html#eagerx.core.entities.Node.make): Makes the parameter specification of the node.\n",
    "- [`initialize()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/node.html#eagerx.core.entities.Node.initialize): Initializes the node.\n",
    "- [`reset()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/node.html#eagerx.core.entities.Node.reset): Resets the node at the beginning of an episode.\n",
    "- [`callback()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/node.html#eagerx.core.entities.Node.callback): Called at the rate of the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878e4395-cb31-40ed-9974-b4b82ae0c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eagerx import register, Space\n",
    "from eagerx.core.specs import NodeSpec\n",
    "from eagerx.utils.utils import Msg\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Overlay(eagerx.Node):\n",
    "    @classmethod\n",
    "    def make(\n",
    "        cls,\n",
    "        name: str,\n",
    "        rate: float,\n",
    "        process: int = eagerx.ENVIRONMENT,\n",
    "        color: str = \"cyan\",\n",
    "    ) -> NodeSpec:\n",
    "        \"\"\"Overlay spec\"\"\"\n",
    "        # Get base parameter specification with defaults parameters\n",
    "        spec = cls.get_specification()\n",
    "\n",
    "        # Adjust default params\n",
    "        spec.config.update(name=name, rate=rate, process=process, color=color)\n",
    "        spec.config.update(inputs=[\"base_image\", \"u\", \"theta\"], outputs=[\"image\"])\n",
    "        return spec\n",
    "        \n",
    "    def initialize(self, spec: NodeSpec):\n",
    "        \"\"\"Nothing to initialize\"\"\"\n",
    "        pass\n",
    "\n",
    "    @register.states()\n",
    "    def reset(self):\n",
    "        \"\"\"Nothing to reset (i.e. stateless node)\"\"\"\n",
    "        pass\n",
    "\n",
    "    @register.inputs(\n",
    "        base_image=Space(dtype=\"uint8\"),\n",
    "        u=Space(low=[-2], high=[2]),\n",
    "        theta=Space(shape=(), dtype=\"float32\"),\n",
    "    )\n",
    "    @register.outputs(image=Space(dtype=\"uint8\"))\n",
    "    def callback(self, t_n: float, base_image: Msg, u: Msg, theta: Msg):\n",
    "        if len(base_image.msgs[-1].data) > 0:\n",
    "            u = u.msgs[-1].data[0] if u else 0\n",
    "            theta = theta.msgs[-1]\n",
    "\n",
    "            # Set background image from base_image\n",
    "            img = base_image.msgs[-1]\n",
    "            height, width, _ = img.shape\n",
    "            side_length = min(width, height)\n",
    "\n",
    "            # Put text\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            text = \"Applied Voltage\"\n",
    "            text_size = cv2.getTextSize(text, font, 0.5, 2)[0]\n",
    "            text_x = int((width - text_size[0]) / 2)\n",
    "            text_y = int(text_size[1])\n",
    "            img = cv2.putText(img, text, (text_x, text_y), font, 0.5, (0, 0, 0))\n",
    "\n",
    "            # Draw grey bar\n",
    "            img = cv2.rectangle(\n",
    "                img,\n",
    "                (width // 2 - side_length * 4 // 10, height // 2 - side_length * 9 // 20),\n",
    "                (width // 2 + 4 * side_length // 10, height // 2 - 4 * side_length // 10),\n",
    "                (125, 125, 125),\n",
    "                -1,\n",
    "            )\n",
    "\n",
    "            # Fill bar proportional to the action that is applied\n",
    "            p1 = (width // 2, height // 2 - side_length * 9 // 20)\n",
    "            p2 = (width // 2 + int(side_length * u * 2 / 15), height // 2 - 4 * side_length // 10)\n",
    "            img = cv2.rectangle(img, p1, p2, (0, 0, 0), -1)\n",
    "\n",
    "            # START EXERCISE 1.3\n",
    "\n",
    "            # START EXERCISE 1.3\n",
    "\n",
    "            # Add theta info\n",
    "            img = cv2.putText(\n",
    "                img, f\"theta ={theta: .2f} rad\", (text_x, height - int(2.2 * text_y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0)\n",
    "            )\n",
    "\n",
    "            return dict(image=img)\n",
    "        else:\n",
    "            return dict(image=np.zeros((0, 0, 3), dtype=\"uint8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e352c4-b765-4f3b-b870-358abe3b3916",
   "metadata": {},
   "source": [
    "Below, we wil initially render the raw images produced by the pendulum image sensor as we did in the preceding tutorials. In the exercise of this tutorial, we will:\n",
    "- Add the overlay node to the graph.\n",
    "- Connect the inputs of the overlay node. I.e. `u` to the `voltage` action and `pendulum.sensors.image` to the `overlay.inputs.base_image`.\n",
    "- Change the render source from `pendulum.sensors.image` to `overlay.outputs.image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858c3f13-6c63-4f92-88d3-872220769fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the overlay node parameter specification\n",
    "overlay = Overlay.make(\"overlay\", rate)\n",
    "\n",
    "# Copy the spaces of the actuators and sensors to the overlay node.\n",
    "overlay.inputs.u.space = pendulum.actuators.u.space\n",
    "overlay.inputs.base_image.space = pendulum.sensors.image.space\n",
    "overlay.outputs.image.space = pendulum.sensors.image.space\n",
    "    \n",
    "# Add overlay node to graph and connect it\n",
    "# START EXERCISE 1.1\n",
    "# Add the overlay node to the graph and connect the following:\n",
    "# - pendulum.sensors.image to overlay.inputs.base_image \n",
    "# - action \"voltage\" to overlay.inputs.u\n",
    "# - pendulum.sensors.theta to overlay.inputs.theta\n",
    "\n",
    "# END EXERCISE 1.1\n",
    "\n",
    "# Define the render source\n",
    "# START EXERCISE 1.2\n",
    "# Change the render source to overlay.outputs.image.\n",
    "graph.render(source=pendulum.sensors.image, rate=rate)\n",
    "# START EXERCISE 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f13bf73c-827d-485f-8090-d10ebd38dafc",
   "metadata": {
    "cellView": "form",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown Next, we will define the environment.\n",
    "\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PendulumEnv(eagerx.BaseEnv):\n",
    "    def __init__(self, name: str, rate: float, graph: eagerx.Graph, engine: eagerx.Engine):\n",
    "        \"\"\"Initializes an environment with EAGERx dynamics.\n",
    "\n",
    "        :param name: The name of the environment. Everything related to this environment\n",
    "                     (parameters, topics, nodes, etc...) will be registered under namespace: \"/[name]\".\n",
    "        :param rate: The rate (Hz) at which the environment will run.\n",
    "        :param graph: The graph consisting of nodes and objects that describe the environment's dynamics.\n",
    "        :param engine: The physics engine that will govern the environment's dynamics.\n",
    "        \"\"\"\n",
    "        # Make the backend specification\n",
    "        from eagerx.backends.single_process import SingleProcess\n",
    "        backend = SingleProcess.make()\n",
    "        \n",
    "        self.eval = eval\n",
    "        \n",
    "        # Maximum episode length\n",
    "        self.max_steps = 100\n",
    "        \n",
    "        # Step counter\n",
    "        self.steps = None\n",
    "        super().__init__(name, rate, graph, engine, backend, force_start=True)\n",
    "    \n",
    "    def step(self, action: Dict):\n",
    "        \"\"\"A method that runs one timestep of the environment's dynamics.\n",
    "\n",
    "        :params action: A dictionary of actions provided by the agent.\n",
    "        :returns: A tuple (observation, reward, done, info).\n",
    "\n",
    "            - observation: Dictionary of observations of the current timestep.\n",
    "\n",
    "            - reward: amount of reward returned after previous action\n",
    "\n",
    "            - done: whether the episode has ended, in which case further step() calls will return undefined results\n",
    "\n",
    "            - info: contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "        \"\"\"\n",
    "        # Take step\n",
    "        observation = self._step(action)\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Get angle and angular velocity\n",
    "        # Take first element because of window size (covered in other tutorial)\n",
    "        th = observation[\"angle\"][0]\n",
    "        thdot = observation[\"angular_velocity\"][0]\n",
    "\n",
    "        # Convert from numpy array to float\n",
    "        u = float(action[\"voltage\"])\n",
    "\n",
    "        # Calculate cost\n",
    "        # Penalize angle error, angular velocity and input voltage\n",
    "        cost = th**2 + 0.1 * (thdot / (1 + 10 * abs(th))) ** 2 + 0.01 * u ** 2  \n",
    "\n",
    "        # Determine when is the episode over\n",
    "        # currently just a timeout after 100 steps\n",
    "        done = self.steps > self.max_steps\n",
    "\n",
    "        # Set info, tell the algorithm the termination was due to a timeout\n",
    "        # (the episode was truncated)\n",
    "        info = {\"TimeLimit.truncated\": self.steps > self.max_steps}\n",
    "        \n",
    "        return observation, -cost, done, info\n",
    "    \n",
    "    def reset(self) -> Dict:\n",
    "        \"\"\"Resets the environment to an initial state and returns an initial observation.\n",
    "\n",
    "        :returns: The initial observation.\n",
    "        \"\"\"\n",
    "        # Determine reset states\n",
    "        states = self.state_space.sample()\n",
    "            \n",
    "        # Perform reset\n",
    "        observation = self._reset(states)\n",
    "\n",
    "        # Reset step counter\n",
    "        self.steps = 0\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28cb4b-a564-43c9-98bb-c23259e5369f",
   "metadata": {},
   "source": [
    "Next, we will intially use the OdeEngine, as we did in the preceding tutorials. Later on in the exercises. we will switch and use the [GymEngine](https://github.com/eager-dev/eagerx/blob/master/eagerx/engines/openai_gym/engine.py) instead.\n",
    "\n",
    "Because the overlay node is agnostic to the *engine-specific* implementation of the pendulum object, it will naturally overlay the additional information on whatever image it receives. Hence, this allows informative rendering to be available, whatever physics-engine is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18b5438",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the two supported engines\n",
    "from eagerx_ode.engine import OdeEngine\n",
    "from eagerx.engines.openai_gym.engine import GymEngine\n",
    "\n",
    "# Make the engine\n",
    "# START EXERCISE 1.4\n",
    "engine = OdeEngine.make(rate=rate)\n",
    "# engine = GymEngine.make(rate=rate, process=eagerx.ENVIRONMENT)\n",
    "# END EXERCISE 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a4dd8-9a01-47fc-bbf8-b24a6e716a8e",
   "metadata": {},
   "source": [
    "At this point, we have create a graph containing the pendulum. We provide the graph to the environment together with the engine. Based on this engine, we will initialize the *engine-specific implementation* for the pendulum that was registered with this engine. \n",
    "- If we use the [OdeEngine](https://github.com/eager-dev/eagerx_ode), the raw sensor images are produced by the custom render function in the registered OdeEngine implementation [here](https://github.com/eager-dev/eagerx_tutorials/blob/3ddc2eb7558c7825095611fec3a01a47f5e7af79/eagerx_tutorials/pendulum/objects.py#L108-L168).\n",
    "- If we use the [GymEngine](https://github.com/eager-dev/eagerx/blob/master/eagerx/engines/openai_gym/engine.py), the raw sensor images are produced by the [Pendulum-v1](https://gym.openai.com/envs/Pendulum-v0/) environment.\n",
    "- If we would have an implemention for the real-world and registered it with the [RealEngine](https://github.com/eager-dev/eagerx_reality/blob/m1aster/eagerx_reality/engine.py), the raw sensor images could, for example, be produced by a real camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d9d2bf3-8a66-4605-a31f-e785e0a17b0b",
   "metadata": {
    "cellView": "form",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[WARN]: Backend 'SINGLE_PROCESS' does not support multiprocessing, so all nodes are launched in the ENVIRONMENT process.\u001b[0m\n",
      "action_space:  Dict(voltage:Space([-2.], [2.], (1,), float32))\n",
      "observation_space:  Dict(angle:Box([-999.], [999.], (1,), float32), angular_velocity:Box([-999.], [999.], (1,), float32))\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -976     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 404      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 1.67     |\n",
      "|    ent_coef        | 0.914    |\n",
      "|    ent_coef_loss   | -0.139   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 303      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -941     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 808      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 33.6     |\n",
      "|    critic_loss     | 0.332    |\n",
      "|    ent_coef        | 0.818    |\n",
      "|    ent_coef_loss   | -0.256   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 707      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -910     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 1212     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48       |\n",
      "|    critic_loss     | 0.677    |\n",
      "|    ent_coef        | 0.746    |\n",
      "|    ent_coef_loss   | -0.272   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1111     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -886     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 1616     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 62.4     |\n",
      "|    critic_loss     | 2.95     |\n",
      "|    ent_coef        | 0.695    |\n",
      "|    ent_coef_loss   | -0.211   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1515     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -869     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 2020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 78.5     |\n",
      "|    critic_loss     | 4.22     |\n",
      "|    ent_coef        | 0.657    |\n",
      "|    ent_coef_loss   | -0.169   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1919     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -858     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 2424     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 90.9     |\n",
      "|    critic_loss     | 2.37     |\n",
      "|    ent_coef        | 0.628    |\n",
      "|    ent_coef_loss   | -0.148   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2323     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -849     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 2828     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 104      |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.602    |\n",
      "|    ent_coef_loss   | -0.0663  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2727     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -842     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 3232     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 118      |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.568    |\n",
      "|    ent_coef_loss   | -0.0937  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3131     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -836     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 3636     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 133      |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.521    |\n",
      "|    ent_coef_loss   | -0.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3535     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -833     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 4040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 146      |\n",
      "|    critic_loss     | 0.598    |\n",
      "|    ent_coef        | 0.463    |\n",
      "|    ent_coef_loss   | -0.377   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3939     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -831     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 4444     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 160      |\n",
      "|    critic_loss     | 0.516    |\n",
      "|    ent_coef        | 0.405    |\n",
      "|    ent_coef_loss   | -0.483   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4343     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "#@title Training\n",
    "\n",
    "#@markdown Finally, we train the agent using [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/), again similar to the preceding tutorials.\n",
    "\n",
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from eagerx.wrappers import Flatten\n",
    "from gym.wrappers.rescale_action import RescaleAction\n",
    "\n",
    "# Initialize Environment\n",
    "env = PendulumEnv(name=\"PendulumEnv\", rate=rate, graph=graph, engine=engine)\n",
    "\n",
    "# Print action & observation space\n",
    "print(\"action_space: \", env.action_space)\n",
    "print(\"observation_space: \", env.observation_space)\n",
    "\n",
    "# Stable Baselines3 expects flattened actions & observations\n",
    "# Convert observation and action space from Dict() to Box(), normalize actions\n",
    "env = Flatten(env)\n",
    "env = RescaleAction(env, min_action=-1.0, max_action=1.0)\n",
    "\n",
    "# Check that env follows Gym API and returns expected shapes\n",
    "check_env(env)\n",
    "\n",
    "# Toggle render\n",
    "env.render(\"human\")\n",
    "\n",
    "# Initialize learner\n",
    "model = sb3.SAC(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train for 1 minute (sim time)\n",
    "model.learn(total_timesteps=int(150 * rate))\n",
    "\n",
    "env.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872df1d1-bed3-4b09-80dd-44c8b71a502d",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In this exercise you will create a node that overlays the applied actions over raw images that are produced by the image sensor of the pendulum. As the overlay node is agnostic to the physics-engine, we have the same overlay in every physics-engine.\n",
    "\n",
    "For this exercise, you will need to modify or add some lines of code in the cells above.\n",
    "These lines are indicated by the following comments:\n",
    "\n",
    "```python\n",
    "# START EXERCISE [BLOCK_NUMBER]\n",
    "\n",
    "# END EXERCISE [BLOCK_NUMBER]\n",
    "```\n",
    "\n",
    "However, feel free to play with the other code as well if you are interested.\n",
    "We recommend you to restart and run all code after each section (in Colab there is the option *Restart and run all* under *Runtime*).\n",
    "\n",
    "## 1. Render more informative images\n",
    "\n",
    "\n",
    "### Add your code to the following blocks: \n",
    "\n",
    "1.1 Add the overlay node to the graph and connect the inputs `overlay.inputs.raw_image` and `overlay.inputs.u` to `pendulum.sensors.image` and action `voltage`, respectively.  \n",
    "1.2 Change the render source to `overlay.outputs.image`. Using the [*eagerx_gui* package](https://github.com/eager-dev/eagerx_gui), you would see that the graph looks as below if `graph.gui()` would be called. Run the code, and you should now see the rendered overlay instead of the raw sensor images. \n",
    "\n",
    "<img src=\"./figures/tutorial_6_gui.svg\" width=720>\n",
    "\n",
    "1.3 In the callback of the overlay node, add the current time (i.e. `t_n`) as text to the image. Run the code, and you should see a timestamp that increase while the episode progresses.  \n",
    "1.4 Select the `GymEngine` by uncommenting the marked line. Run the code, and you should see that the base image has changed, but the overlay is nevertheless put on top. Hence, this demonstrates the agnostic behavior of the `graph`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
