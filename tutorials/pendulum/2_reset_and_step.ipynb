{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4c98be-8431-4ded-aec2-32aa5179725e",
   "metadata": {},
   "source": [
    "# Tutorial 2: Reset and Step Function\n",
    "\n",
    "In this tutorial, we will show how to create a gym environment using [EAGERx](https://eagerx.readthedocs.io/en/master/) while specifying the [step function](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv.step_fn) and [reset function](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv.reset_fn).\n",
    "\n",
    "The following will be covered:\n",
    "- Extracting observations in the [step_fn](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv.step_fn)\n",
    "- Resetting states using the [reset_fn](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv.reset_fn)\n",
    "- The `window` argument of the [connect method](https://eagerx.readthedocs.io/en/master/guide/api_reference/graph/graph.html?highlight=connect#eagerx.core.graph.Graph.connect)\n",
    "- Simulating delays using the `delay` argument of the [connect method](https://eagerx.readthedocs.io/en/master/guide/api_reference/graph/graph.html?highlight=connect#eagerx.core.graph.Graph.connect)\n",
    "\n",
    "In the remainder of this tutorial we will go more into detail on these concepts.\n",
    "\n",
    "Furthermore, at the end of this notebook you will find exercises.\n",
    "For the exercises you will have to add/modify a couple of lines of code, which are marked by\n",
    "\n",
    "```python\n",
    "\n",
    "# START EXERCISE [BLOCK_NUMBER]\n",
    "\n",
    "# END EXERCISE [BLOCK_NUMBER]\n",
    "```\n",
    "\n",
    "## Pendulum Swing-up\n",
    "\n",
    "We will create an environment for solving the classic control problem of swinging up an underactuated pendulum, very similar to the [Pendulum-v0 environment](https://gym.openai.com/envs/Pendulum-v0/).\n",
    "Our goal is to swing up this pendulum to the upright position and keep it there, while minimizing the velocity of the pendulum and the input voltage.\n",
    "\n",
    "Since the dynamics of a pendulum actuated by a DC motor are well known, we can simulate the pendulum by integrating the corresponding ordinary differential equations (ODEs):\n",
    "\n",
    "\n",
    "$\\mathbf{x} = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix} \\\\ \\dot{\\mathbf{x}} = \\begin{bmatrix} \\dot{\\theta} \\\\ \\frac{1}{J}(\\frac{K}{R}u - mgl \\sin{\\theta} - b \\dot{\\theta} - \\frac{K^2}{R}\\dot{\\theta})\\end{bmatrix}$\n",
    "\n",
    "with $\\theta$ the angle w.r.t. upright position, $\\dot{\\theta}$ the angular velocity, $u$ the input voltage, $J$ the inertia, $m$ the mass, $g$ the gravitational constant, $l$ the length of the pendulum, $b$ the motor viscous friction constant, $K$ the motor constant and $R$ the electric resistance.\n",
    "\n",
    "## Notebook Setup\n",
    "\n",
    "In order to be able to run the code, we need to install the *eagerx_tutorials* package and ROS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985d462b-fdc5-4539-bd18-a58eafd22353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab.\n",
      "Execute ROS commands as \"!...\".\n",
      "ROS noetic available.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import eagerx_tutorials\n",
    "except ImportError:\n",
    "    !{\"echo 'Installing eagerx-tutorials with pip.' && pip install eagerx-tutorials >> /tmp/eagerx_install.txt 2>&1\"}\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !{\"curl 'https://raw.githubusercontent.com/eager-dev/eagerx_tutorials/master/scripts/setup_colab.sh' > ~/setup_colab.sh\"}\n",
    "    !{\"bash ~/setup_colab.sh\"}\n",
    "\n",
    "# Setup interactive notebook\n",
    "# Required in interactive notebooks only.\n",
    "from eagerx_tutorials import helper\n",
    "helper.setup_notebook()\n",
    "env = None\n",
    "\n",
    "# Allows reloading of registered entites from changed files\n",
    "# Required in interactive notebooks only.\n",
    "%reload_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d44bf-fbe9-4f22-a4ab-15c40decb6ad",
   "metadata": {},
   "source": [
    "## Let's get started\n",
    "\n",
    "We start by importing the required packages and initializing EAGERx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d59319-5b5f-4760-8c19-cc3279cac896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... logging to /home/jelle/.ros/log/d734afb2-c7c2-11ec-ab25-bdefe663dbb0/roslaunch-jelle-Alienware-m15-R4-63389.log\n",
      "\u001b[1mstarted roslaunch server http://145.94.60.89:33347/\u001b[0m\n",
      "ros_comm version 1.15.14\n",
      "\n",
      "\n",
      "SUMMARY\n",
      "========\n",
      "\n",
      "PARAMETERS\n",
      " * /rosdistro: noetic\n",
      " * /rosversion: 1.15.14\n",
      "\n",
      "NODES\n",
      "\n",
      "[INFO] [1651241088.184788]: Roscore cannot run as another roscore/master is already running. Continuing without re-initializing the roscore.\n"
     ]
    }
   ],
   "source": [
    "import eagerx\n",
    "import eagerx_tutorials.pendulum  # Registers Pendulum\n",
    "import eagerx_ode  # Registers OdeBridge\n",
    "\n",
    "# Initialize eagerx (starts roscore if not already started.)\n",
    "eagerx.initialize(\"eagerx_core\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df883914-af6b-4d82-a1a7-6045b70347ff",
   "metadata": {},
   "source": [
    "Next, we make the *Pendulum* object and add it to an empty graph, just like we did in the [first tutorial](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/pendulum_1.ipynb).\n",
    "\n",
    "We will again connect the *u* actuator of the *Pendulum* to an action that we will call *voltage* and connect the sensors *theta* and *dtheta* to observations, which we will call *angle* and *angular_velocity*.\n",
    "However, we will now go a bit more into detail on the [connect method](https://eagerx.readthedocs.io/en/master/guide/api_reference/graph/graph.html?highlight=connect#eagerx.core.graph.Graph.connect).\n",
    "When connecting outputs, sensors or actions, we can specify among other things the `window` of the connection.\n",
    "It specifies how to deal with messages that are sent between nodes in between calls to their callback.\n",
    "In some cases it makes sense to use the last one only; in others you would like to receive all messages between calls.\n",
    "This can be achieved by setting the `window` size:\n",
    "\n",
    "- `window` $= 1$: Only the last received input message are available to the receiver.\n",
    "- `window` $= x \\ge 1$: The trailing last $x$ received input messages are available to the receiver ($1 \\le$ received number of messages $\\le$ `window` ).\n",
    "- `window` $= 0$: All input messages received since the last call to the node's callback are available.\n",
    "\n",
    "This is in particular relevant when connecting to observations, since it has consequences for the size of the observation space.\n",
    "When connecting to an observation with `window` $= 0$, this observation will **not** be included in the observation space of the agent, because its dimensions might change every time step and are therefore unknown on beforehand.\n",
    "Also worth noting, is that for observations if `window` $= x > 1$, at time step $t < x$, the first message is repeated $x - t$ times to ensure that the dimensions of the observation space are consistent.\n",
    "\n",
    "Next to the `window` size, we can also specify the `delay` of each connection.\n",
    "In this way, we can easily simulate delays for inputs and sensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca2e099b-1723-45d4-9ade-44ec3475002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rate (Hz)\n",
    "rate = 30.0\n",
    "\n",
    "# Initialize empty graph\n",
    "graph = eagerx.Graph.create()\n",
    "\n",
    "# Make pendulum\n",
    "\n",
    "# START EXERCISE 1.1\n",
    "sensors = [\"theta\", \"dtheta\", \"image\"]\n",
    "# END EXERCISE 1.1\n",
    "\n",
    "# START EXERCISE 2.1\n",
    "states = [\"model_state\"]\n",
    "# END EXERCISE 2.1\n",
    "\n",
    "pendulum = eagerx.Object.make(\"Pendulum\", \"pendulum\", actuators=[\"u\"], sensors=sensors, states=states)\n",
    "\n",
    "# Add pendulum to the graph\n",
    "graph.add(pendulum)\n",
    "\n",
    "# Connect the pendulum to an action and observation\n",
    "# We will now explicitly set the window size\n",
    "graph.connect(action=\"voltage\", target=pendulum.actuators.u, window=1)\n",
    "graph.connect(source=pendulum.sensors.theta, observation=\"angle\", window=1)\n",
    "\n",
    "# START EXERCISE 1.2\n",
    "graph.connect(source=pendulum.sensors.dtheta, observation=\"angular_velocity\", window=1)\n",
    "# END EXERCISE 1.2\n",
    "\n",
    "# START EXERCISE 1.3\n",
    "\n",
    "# END EXERCISE 1.3\n",
    "\n",
    "# Render image\n",
    "graph.render(source=pendulum.sensors.image, rate=rate)\n",
    "\n",
    "# Make OdeBridge\n",
    "bridge = eagerx.Bridge.make(\"OdeBridge\", rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad124293-588e-46b4-8f39-99f8461a10a0",
   "metadata": {},
   "source": [
    "Using the [*eagerx_gui* package](https://github.com/eager-dev/eagerx_gui), we see that the graph looks as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "graph.gui()\n",
    "```\n",
    "\n",
    "<img src=\"./figures/tutorial_1_gui.svg\" width=720>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690463d-1998-4095-9f7e-8eced4a4196c",
   "metadata": {},
   "source": [
    "We will now define the [step function](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv.step_fn).\n",
    "Here we define the `reward` and fill the `info` dictionary at each time step.\n",
    "Since we want to stabilize the pendulum in upright position — while minimising the input voltage — we define the reward to be a weighted sum of $\\theta^2$, $\\dot{\\theta^2}$ and $u^2$.\n",
    "\n",
    "We will elaborate a bit more on this step function.\n",
    "The step function is an argument to the [EagerxEnv](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv).\n",
    "This function is called by the EAGERx environment every time step and it returns the same things as the `step()` method of OpenAI Gym environments, i.e. `observation` (**dict**), `reward` (**float**), `done` (**boolean**) and `info` (**dict**).\n",
    "More information on this can be found [here](https://gym.openai.com/docs/#observations).\n",
    "The input to the step function in EAGERx are:\n",
    "\n",
    "- `previous_observation` (**dict**): The `observation` at the previous timestep.\n",
    "- `observation` (**dict**): The `observation` at the current timestep.\n",
    "- `action` (**dict**): The agent's action at the current timestep. \n",
    "- `steps` (**int**): The number of timesteps since the start of the episode (since the last reset).\n",
    "\n",
    "Note that the `observation` is both an input and output of this function and should only be used for extracting information and should not be manipulated.\n",
    "\n",
    "The keys of observations and dictionaries correspond to respectively the value of the `observation` and `action` argument provided in the [connect method](https://eagerx.readthedocs.io/en/master/guide/api_reference/graph/graph.html?highlight=connect#eagerx.core.graph.Graph.connect).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0949c08-00d3-4a9a-9c6f-cb8f304d5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "# Define step function\n",
    "def step_fn(previous_observation: Dict[str, np.ndarray], observation: Dict[str, np.ndarray], action: Dict[str, np.ndarray], steps: int):\n",
    "    \n",
    "    # Get angle \n",
    "    th = observation[\"angle\"][-1]\n",
    "    \n",
    "    # START EXERCISE 1.4\n",
    "    thdot = observation[\"angular_velocity\"][-1]\n",
    "    # END EXERCISE 1.4\n",
    "    \n",
    "    # Convert from numpy array to float\n",
    "    u = float(action[\"voltage\"])\n",
    "    \n",
    "    # Normalize angle so it lies in [-pi, pi]\n",
    "    th -= 2 * np.pi * np.floor((th + np.pi) / (2 * np.pi))\n",
    "    \n",
    "    # Calculate cost\n",
    "    # Penalize angle error, angular velocity and input voltage\n",
    "    cost = th**2 + 0.1 * thdot**2 + 0.001 * u**2\n",
    "    \n",
    "    # Determine when is the episode over\n",
    "    # currently just a timeout after 100 steps\n",
    "    done = steps > 100\n",
    "    \n",
    "    # Set info, tell the algorithm the termination was due to a timeout\n",
    "    # (the episode was truncated)\n",
    "    info = {\"TimeLimit.truncated\": steps > 100}\n",
    "    \n",
    "    return observation, -cost, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba84fa-6223-40a6-b283-aa5646369ae0",
   "metadata": {},
   "source": [
    "Next we will also define a [reset function](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html#eagerx.core.env.EagerxEnv.reset_fn).\n",
    "The reset function allows to specify how states are reset at the beginning of an episode.\n",
    "Remember that we have one object (*Pendulum*) with one state (*model_state*).\n",
    "This *model_state* corresponds to $x = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}$.\n",
    "The default reset function as defined in [EagerxEnv](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html?highlight=eagerxenv#eagerx.core.env.EagerxEnv) is:\n",
    "```python\n",
    "reset_fn = lambda env: env.state_space.sample()\n",
    "```\n",
    "which results in resetting each state to a randomly sampled value from the corresponding state distribution.\n",
    "\n",
    "Here we will create a custom reset function that will reset the pendulum at the beginning of an episode to $\\mathbf{x} = \\begin{bmatrix} \\pi \\\\ 0 \\end{bmatrix}$.\n",
    "This corresponds to the downward position with zero velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9f408f-a84d-4367-b32e-fbdb6e94e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reset function\n",
    "\n",
    "def reset_fn(env: eagerx.EagerxEnv):\n",
    "    state_dict = dict()\n",
    "    \n",
    "    # START EXERCISE 2.2\n",
    "    \n",
    "    # key = \"[object_name]/[state_name]\"\n",
    "    state_dict[\"pendulum/model_state\"] = np.array([np.pi, 0], dtype=\"float32\")\n",
    "    \n",
    "    # END EXERCISE 2.2\n",
    "    \n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da4afe-7163-4540-81a5-5793bf6225c5",
   "metadata": {},
   "source": [
    "Finally, we will initialize the environment and train the agent using [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/), similar to the first tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf343ae-0cb1-4726-b03d-f80830e61784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1651241088.944307]: Node \"/PendulumEnv/env/supervisor\" initialized.\n",
      "[INFO] [1651241089.087094]: Node \"/PendulumEnv/bridge\" initialized.\n",
      "[INFO] [1651241089.209922]: Node \"/PendulumEnv/environment\" initialized.\n",
      "[INFO] [1651241089.236116]: Node \"/PendulumEnv/env/render\" initialized.\n",
      "[INFO] [1651241089.311365]: Node \"/PendulumEnv/pendulum/theta\" initialized.\n",
      "[INFO] [1651241089.383828]: Node \"/PendulumEnv/pendulum/dtheta\" initialized.\n",
      "[INFO] [1651241089.454643]: START RENDERING!\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "[INFO] [1651241089.483672]: Adding object \"pendulum\" of type \"Pendulum\" to the simulator.\n",
      "[INFO] [1651241089.501378]: Node \"/PendulumEnv/pendulum/x\" initialized.\n",
      "[INFO] [1651241089.512113]: [pendulum/image] START RENDERING!\n",
      "[INFO] [1651241089.518910]: Node \"/PendulumEnv/pendulum/image\" initialized.\n",
      "[INFO] [1651241089.533136]: Node \"/PendulumEnv/pendulum/pendulum_actuator\" initialized.\n",
      "[INFO] [1651241089.547454]: Node \"/PendulumEnv/pendulum/u\" initialized.\n",
      "[INFO] [1651241092.752754]: Nodes initialized.\n",
      "[INFO] [1651241092.810296]: Pipelines initialized.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -974     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 76       |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 404      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.1     |\n",
      "|    critic_loss     | 5.39     |\n",
      "|    ent_coef        | 0.917    |\n",
      "|    ent_coef_loss   | -0.116   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 303      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -967     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 808      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32.6     |\n",
      "|    critic_loss     | 3.2      |\n",
      "|    ent_coef        | 0.823    |\n",
      "|    ent_coef_loss   | -0.258   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 707      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -958     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 1212     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.5     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.727    |\n",
      "|    ent_coef_loss   | -0.437   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1111     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -931     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 1616     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 62       |\n",
      "|    critic_loss     | 0.991    |\n",
      "|    ent_coef        | 0.648    |\n",
      "|    ent_coef_loss   | -0.488   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1515     |\n",
      "---------------------------------\n",
      "[INFO] [1651241117.890363]: [PendulumEnv][/PendulumEnv/bridge] Shutting down.\n",
      "[INFO] [1651241117.891250]: [/PendulumEnv/bridge] Shutting down '/PendulumEnv/pendulum/x'.\n",
      "[INFO] [1651241117.891805]: [/PendulumEnv/pendulum/x] Shutting down.\n",
      "[INFO] [1651241117.945183]: [/PendulumEnv/bridge] Shutting down '/PendulumEnv/pendulum/image'.\n",
      "[INFO] [1651241117.945928]: [/PendulumEnv/pendulum/image] Shutting down.\n",
      "[INFO] [1651241117.946558]: [/PendulumEnv/bridge] Shutting down '/PendulumEnv/pendulum/pendulum_actuator'.\n",
      "[INFO] [1651241117.947154]: [/PendulumEnv/pendulum/pendulum_actuator] Shutting down.\n",
      "[INFO] [1651241117.947795]: [/PendulumEnv/bridge] Shutting down '/PendulumEnv/pendulum/u'.\n",
      "[INFO] [1651241117.948419]: [/PendulumEnv/pendulum/u] Shutting down.\n",
      "[INFO] [1651241117.949280]: [/PendulumEnv/bridge] Shutting down.\n",
      "[INFO] [1651241117.950547]: [PendulumEnv][/PendulumEnv/env/render] Shutting down.\n",
      "[INFO] [1651241117.951180]: [/PendulumEnv/env/render] Shutting down.\n",
      "[INFO] [1651241117.953123]: [PendulumEnv][/PendulumEnv/pendulum/theta] Shutting down.\n",
      "[INFO] [1651241117.953688]: [/PendulumEnv/pendulum/theta] Shutting down.\n",
      "[INFO] [1651241117.954355]: [PendulumEnv][/PendulumEnv/pendulum/dtheta] Shutting down.\n",
      "[INFO] [1651241117.954935]: [/PendulumEnv/pendulum/dtheta] Shutting down.\n",
      "[INFO] [1651241117.955613]: [/PendulumEnv/env/supervisor] Shutting down.\n",
      "[INFO] [1651241117.958478]: [/PendulumEnv/environment] Shutting down.\n",
      "[INFO] [1651241117.960480]: Parameters under namespace \"/PendulumEnv\" deleted.\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3 as sb\n",
    "from eagerx.wrappers import Flatten\n",
    "\n",
    "# Initialize Environment\n",
    "env = eagerx.EagerxEnv(name=\"PendulumEnv\", rate=rate, graph=graph, bridge=bridge, step_fn=step_fn, reset_fn=reset_fn)\n",
    "\n",
    "# Toggle render\n",
    "env.render(\"human\")\n",
    "\n",
    "# Stable Baselines3 expects flattened actions & observations\n",
    "# Convert observation and action space from Dict() to Box()\n",
    "env = Flatten(env)\n",
    "\n",
    "# Initialize learner\n",
    "model = sb.SAC(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train for 1 minute (sim time)\n",
    "model.learn(total_timesteps=int(60 * rate))\n",
    "\n",
    "env.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd13fcd-7275-40a6-b779-3caef149dd55",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "In these exercises we will address the scenario in which the angular velocity $\\dot{\\theta}$ is not available.\n",
    "So now we want to learn to swing up the pendulum using the angle $\\theta$ only.\n",
    "Step-by-step we will guide you through the process of creating an environment for this scenario in the following exercises.\n",
    "\n",
    "For these exercises, you will need to modify or add some lines of code in the cells above.\n",
    "These lines are indicated by the following comments:\n",
    "\n",
    "```python\n",
    "# START EXERCISE [BLOCK_NUMBER]\n",
    "\n",
    "# END EXERCISE [BLOCK_NUMBER]\n",
    "```\n",
    "\n",
    "However, feel free to play with the other code as well if you are interested.\n",
    "We recommend you to restart and run all code after each section (in Colab there is the option *Restart and run all* under *Runtime*).\n",
    "\n",
    "\n",
    "## 1. Violation of the Markov property\n",
    "\n",
    "We could naively remove the sensor *dtheta* in the environment above and train the agent.\n",
    "However, this will most likely not result in a successful policy because the [Markov property](http://www.incompleteideas.net/book/3/node6.html) is violated.\n",
    "It is not possible to fully restore the Markov property without observing $\\dot{\\theta}$, but we can create a representation that is sufficient for solving the task.\n",
    "If we stack the last three measurements of $\\theta$ and provide this information as an observation, the agent will be able to approximate $\\dot{\\theta}$ (e.g. using a finite difference method).\n",
    "With this information, the agent can estimate the angular velocity at the previous time step.\n",
    "If we also provide the last applied action as an observation, the agent will be able to estimate $\\dot{\\theta}$ at the current time step.\n",
    "\n",
    "After this the graph should look as follows:\n",
    "\n",
    "<img src=\"./figures/tutorial_21_gui.svg\" width=720>\n",
    "\n",
    "Furthermore, the Markov property can also be violated due to delays.\n",
    "If we want our policy to transfer from simulation to a real system, we also need to account for delays that are present in the real world.\n",
    "Therefore we will simulate that the sensor *theta* has a delay of 0.01 seconds.\n",
    "\n",
    "We also have to update `step_fn`, since we no longer have the *angular_velocity* observation.\n",
    "In the reward function, we still want to penalize the angular velocity.\n",
    "Therefore we will have to approximate $\\dot{\\theta}$ in `step_fn`, which could for example be done as follows: $\\hat{\\dot{\\theta}} = \\text{rate} \\times (\\theta_k  - \\theta_{k - 1})$ where $k$ is the time step and rate is the `rate` of the [environment](https://eagerx.readthedocs.io/en/master/guide/api_reference/env/index.html).\n",
    "\n",
    "### Add your code to the following blocks: \n",
    "\n",
    "1.1 Remove sensor *dtheta* from and add sensor *u* to the list of sensors.  \n",
    "1.2 Connect sensor *theta* with `window` = 3 to stack the last three observations of $\\theta$ and set `delay` to 0.01.  \n",
    "1.3 Connect *u* to an observation called *action_applied* with `window` = 1.  \n",
    "1.4 Update the`step_fn` such that we use an estimate of $\\dot{\\theta}$ to calculate the reward.\n",
    "Hint: you could use the `previous_observation` for this.\n",
    "\n",
    "## 2. Initial state sampling and domain randomization\n",
    "\n",
    "Next, we will add domain randomization [domain randomization](https://sites.google.com/view/domainrandomization/), in order to improve the robustness against model inacurracies.\n",
    "If we want to transfer a policy from simulation to a real system, we need to be aware that the model used for simulation is inaccurate and that the agent could possibly exploit these inaccuracies.\n",
    "One of the techniques for addressing this problem is domain randomization, i.e. varying over simulator parameters in order to improve the robustness of the resulting policy.\n",
    "More specifically, we will do this by varying over the ODE parameters ($m, l, J, b, K$ and $R$).\n",
    "We can do this by adding the *model_parameters* state.\n",
    "\n",
    "After this the graph should look as follows:\n",
    "\n",
    "<img src=\"./figures/tutorial_22_gui.svg\" width=720>\n",
    "\n",
    "We will also improve the reset procedure of the environment.\n",
    "At the beginning of each episode, the environment is reset.\n",
    "In the code as provided above, the pendulum is reset to the downward position with zero velocity each episode.\n",
    "However, the initial state distribution can have a significant influence on the learning speed.\n",
    "If we sample the $x_0 = \\begin{bmatrix} \\pi \\\\ 0 \\end{bmatrix}$ initial state every time, it will take many timesteps for the agent to obtain experience for $-\\frac{\\pi}{2} < \\theta < \\frac{\\pi}{2}$.\n",
    "Namely, in the beginning the policy will be random and it is unlikely that acting randomly will result in the pendulum gaining enough momentum to move upwards.\n",
    "This is problematic, since the agent will obtain the highest rewards when the pendulum is pointed upwards.\n",
    "If the agent does not explore enough (see [the exploration-exploitation trade-off](http://www.incompleteideas.net/book/2/node2.html)), the agent will not know that it can obtain the highest rewards by swinging the pendulum upward.\n",
    "Therefore, we will update the `reset_fn`, such that we sample the initial state randomly, rather than sampling $x_0 = \\begin{bmatrix} \\pi \\\\ 0 \\end{bmatrix}$ everytime.\n",
    "We also need to make sure that the aforementioned *model_parameters* state that is reset to perform domain randomization.\n",
    "\n",
    "### Add your code to the following blocks: \n",
    "\n",
    "2.1 Add the state *model_parameters* to the list of states of the pendulum  \n",
    "2.2 Update the reset function, such that the *model_state* and *model_parameters* states are reset to random values at the beginning of each episode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
